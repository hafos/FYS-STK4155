#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: simon hille
"""

import numpy as np

from sklearn import linear_model
from sklearn.utils import shuffle

from autograd import grad

class LinearRegression:
    """ Class for performing linear regression methods to fit 2D datasets with higher order polynomials"""
    def __init__(self, X_train = None, trainval = None):
        """ Constructor for generating an instance of the class.
        Arguments
        ---------
        X_train: array
            train data values
        trainval: array
            train function values
        Cost func: func
            If one wants to use gradient decent, a cost function has to be provided
        
        Errors
        ------
        TypeError:
            If no data or order is provided
        """
        if X_train is None or trainval is None:
            raise TypeError(f'Class needs data as Input: <X_train> and <trainval> not provided')	
        self.X_train = X_train
        self.trainval = trainval
        
    def ols(self):
        X_train = self.X_train
        trainval = self.trainval
        beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ trainval
        return beta
    
    def ridge(self, hyperparam=0):
        X_train = self.X_train
        z_train = self.z_train
        I = np.identity(X_train.T.shape[0])
        beta = np.linalg.pinv(X_train.T @ X_train + hyperparam*I) 
        beta = beta @ X_train.T @ z_train
        return beta
    
    def lasso(self, hyperparam=0):
        X_train = self.X_train
        z_train = self.z_train
        lasso_regression = linear_model.Lasso(alpha=hyperparam, max_iter=int(1e6), tol=3e-2, fit_intercept=False)
        lasso_regression.fit(X_train, z_train)
        beta = lasso_regression.coef_
        return beta

class GradDecent:
    def __init__(self, X_train = None, trainval = None, CostFunc = None,
                 learningrate= 0.1, beta = None):
        """
        Arguments
        ---------
        X_train: array
            train data values
        trainval: array
            train function values
        Cost func: func
            If one wants to use gradient decent, a cost function has to be provided
        learningrate: float
            Define the starting learning rate (default: 0.1)
        beta: float
            Define the starting beta (default is generated by np.random.rand)
        
        Errors
        ------
        TypeError:
            If no data is provided
            If no cost function is provided for gradient decent
        Index Error:
            Dimension of given starting beta is wrong
        """
        if X_train is None or trainval is None:
            raise TypeError(f'Class needs data as Input: <X_train> and <trainval> not provided')	
        if CostFunc is None: raise TypeError(f'Cost func is missing')
        
        #ensures reproducibility
        np.random.seed(1999)
        if beta is None: beta = np.random.randn(X_train.shape[1],1)
        if beta.shape[0] != X_train.shape[1]: raise IndexError('dim. of beta is wrong')
        
        self.X_train = X_train
        self.trainval = trainval
        self.CostFunc = CostFunc
        self.learningrate = learningrate
        self.beta = beta
        self.train_grad = grad(CostFunc,2)
        
    def const(self, iterations = 100):
        X_train = self.X_train
        trainval = self.trainval
        beta = self.beta.copy()
        #POSITION NEEDS TO BE GENERALIZED LATER
        for itera in range(iterations):
            gradient = self.train_grad(trainval,X_train,beta)
            beta -= self.learningrate*gradient
        return beta
    
    def momentum(self, iterations = 100, delta_momentum = 0.3):
        X_train = self.X_train
        trainval = self.trainval
        beta = self.beta.copy()
        change = 0
        for itera in range(iterations):
            gradient = self.train_grad(trainval,X_train,beta)
            new_change = self.learningrate*gradient+delta_momentum*change
            beta -= new_change
            change = new_change
        return beta
    
class StochGradDecent:
    def __init__(self, X_train = None, trainval = None, CostFunc = None,
                 learningrate= 0.1, beta = None):
        """
        Arguments
        ---------
        X_train: array
            train data values
        trainval: array
            train function values
        Cost func: func
            If one wants to use gradient decent, a cost function has to be provided
        learningrate: float
            Define the starting learning rate (default: 0.1)
        beta: float
            Define the starting beta (default is generated by np.random.rand
        n_epochs: int
            ... (default: 50)
        batchsize: int
            ... (default: 5)
        
        Errors
        ------
        TypeError:
            If no data is provided
            If no cost function is provided for gradient decent
        Index Error:
            Dimension of given starting beta is wrong
        """
        if X_train is None or trainval is None:
            raise TypeError(f'Class needs data as Input: <X_train> and <trainval> not provided')	
        if CostFunc is None: raise TypeError(f'Cost func is missing')
        
        #ensures reproducibility
        np.random.seed(1999)
        if beta is None: beta = np.random.randn(X_train.shape[1],1)
        if beta.shape[0] != X_train.shape[1]: raise IndexError('dim. of beta is wrong')
        
        self.X_train = X_train
        self.trainval = trainval
        self.CostFunc = CostFunc
        self.learningrate = learningrate
        self.beta = beta
        self.train_grad = grad(CostFunc,2)
        
    def const(self,iterations = 100, batchsize = 4):
        X_train = self.X_train.copy()
        trainval = self.trainval.copy()
        beta = self.beta.copy()
        
        numofbatches = int(X_train.shape[0] / batchsize)
        X_train = np.array_split(X_train,numofbatches,axis=0)
        trainval = np.array_split(trainval,numofbatches)
        for itera in range(iterations):
            rd_ind = np.random.randint(numofbatches)
            gradient = self.train_grad(trainval[rd_ind],X_train[rd_ind],beta)
            beta -= self.learningrate*gradient
        return(beta)
    
    def momentum(self, iterations = 100, batchsize = 4, delta_momentum = 0.3):
        X_train = self.X_train.copy()
        trainval = self.trainval.copy()
        beta = self.beta.copy()
        
        numofbatches = int(X_train.shape[0] / batchsize)
        X_train = np.array_split(X_train,numofbatches,axis=0)
        trainval = np.array_split(trainval,numofbatches)
        
        change = 0
        for itera in range(iterations):
            rd_ind = np.random.randint(numofbatches)
            gradient = self.train_grad(trainval[rd_ind],X_train[rd_ind],beta)
            new_change = self.learningrate*gradient+delta_momentum*change
            beta -= new_change
            change = new_change
        return beta


